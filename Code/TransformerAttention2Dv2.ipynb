{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITF1</th>\n",
       "      <th>ITF2</th>\n",
       "      <th>ITF3</th>\n",
       "      <th>ITF4</th>\n",
       "      <th>ITF5</th>\n",
       "      <th>ITF6</th>\n",
       "      <th>ITF7</th>\n",
       "      <th>ITF8</th>\n",
       "      <th>ITF9</th>\n",
       "      <th>ITF10</th>\n",
       "      <th>...</th>\n",
       "      <th>ITF792</th>\n",
       "      <th>ITF793</th>\n",
       "      <th>ITF794</th>\n",
       "      <th>ITF795</th>\n",
       "      <th>ITF796</th>\n",
       "      <th>ITF797</th>\n",
       "      <th>ITF798</th>\n",
       "      <th>ITF799</th>\n",
       "      <th>ITF800</th>\n",
       "      <th>HLAB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>647.574373</td>\n",
       "      <td>692.800504</td>\n",
       "      <td>738.026634</td>\n",
       "      <td>783.252765</td>\n",
       "      <td>828.478896</td>\n",
       "      <td>873.705026</td>\n",
       "      <td>918.931157</td>\n",
       "      <td>964.157288</td>\n",
       "      <td>1009.383418</td>\n",
       "      <td>1054.609549</td>\n",
       "      <td>...</td>\n",
       "      <td>1682.235546</td>\n",
       "      <td>1637.009415</td>\n",
       "      <td>1591.783284</td>\n",
       "      <td>1546.557154</td>\n",
       "      <td>1501.331023</td>\n",
       "      <td>1456.104892</td>\n",
       "      <td>1410.878762</td>\n",
       "      <td>1365.652631</td>\n",
       "      <td>1320.426500</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>26842.039628</td>\n",
       "      <td>26208.873799</td>\n",
       "      <td>25608.997523</td>\n",
       "      <td>25021.057824</td>\n",
       "      <td>24433.118126</td>\n",
       "      <td>23845.178427</td>\n",
       "      <td>23275.899738</td>\n",
       "      <td>22737.426345</td>\n",
       "      <td>22239.938907</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.905001</td>\n",
       "      <td>87.131131</td>\n",
       "      <td>132.357262</td>\n",
       "      <td>177.583392</td>\n",
       "      <td>222.809523</td>\n",
       "      <td>268.035654</td>\n",
       "      <td>313.261784</td>\n",
       "      <td>358.487915</td>\n",
       "      <td>403.714046</td>\n",
       "      <td>...</td>\n",
       "      <td>236.029627</td>\n",
       "      <td>190.803496</td>\n",
       "      <td>145.577365</td>\n",
       "      <td>100.351235</td>\n",
       "      <td>55.125104</td>\n",
       "      <td>9.898973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9396.837191</td>\n",
       "      <td>9396.837191</td>\n",
       "      <td>9308.550758</td>\n",
       "      <td>9218.098497</td>\n",
       "      <td>9127.646236</td>\n",
       "      <td>9037.193974</td>\n",
       "      <td>8946.741713</td>\n",
       "      <td>8856.289452</td>\n",
       "      <td>8765.837191</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030.277337</td>\n",
       "      <td>2075.503467</td>\n",
       "      <td>2120.729598</td>\n",
       "      <td>2165.955729</td>\n",
       "      <td>2211.181859</td>\n",
       "      <td>2256.407990</td>\n",
       "      <td>2301.634120</td>\n",
       "      <td>2346.860251</td>\n",
       "      <td>2392.086382</td>\n",
       "      <td>2437.312512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>1903.522461</td>\n",
       "      <td>1948.748591</td>\n",
       "      <td>1993.974722</td>\n",
       "      <td>2039.200853</td>\n",
       "      <td>2084.426983</td>\n",
       "      <td>2129.653114</td>\n",
       "      <td>2205.243882</td>\n",
       "      <td>2295.696143</td>\n",
       "      <td>2386.148404</td>\n",
       "      <td>2516.695975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>17660.116352</td>\n",
       "      <td>17705.342482</td>\n",
       "      <td>17750.568613</td>\n",
       "      <td>17795.794744</td>\n",
       "      <td>17841.020874</td>\n",
       "      <td>17886.247005</td>\n",
       "      <td>17931.473136</td>\n",
       "      <td>17976.699266</td>\n",
       "      <td>18021.925397</td>\n",
       "      <td>18067.151528</td>\n",
       "      <td>...</td>\n",
       "      <td>25425.963505</td>\n",
       "      <td>25425.963505</td>\n",
       "      <td>25425.963505</td>\n",
       "      <td>25425.963505</td>\n",
       "      <td>25425.963505</td>\n",
       "      <td>25425.963505</td>\n",
       "      <td>25425.963505</td>\n",
       "      <td>25425.963505</td>\n",
       "      <td>25425.963505</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7367 rows Ã— 801 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ITF1          ITF2          ITF3          ITF4          ITF5  \\\n",
       "0       647.574373    692.800504    738.026634    783.252765    828.478896   \n",
       "1         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "2         0.000000     41.905001     87.131131    132.357262    177.583392   \n",
       "3         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "4      2030.277337   2075.503467   2120.729598   2165.955729   2211.181859   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1923   1903.522461   1948.748591   1993.974722   2039.200853   2084.426983   \n",
       "1924      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1925      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1926  17660.116352  17705.342482  17750.568613  17795.794744  17841.020874   \n",
       "1927      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "              ITF6          ITF7          ITF8          ITF9         ITF10  \\\n",
       "0       873.705026    918.931157    964.157288   1009.383418   1054.609549   \n",
       "1         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "2       222.809523    268.035654    313.261784    358.487915    403.714046   \n",
       "3         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "4      2256.407990   2301.634120   2346.860251   2392.086382   2437.312512   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1923   2129.653114   2205.243882   2295.696143   2386.148404   2516.695975   \n",
       "1924      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1925      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1926  17886.247005  17931.473136  17976.699266  18021.925397  18067.151528   \n",
       "1927      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "      ...        ITF792        ITF793        ITF794        ITF795  \\\n",
       "0     ...   1682.235546   1637.009415   1591.783284   1546.557154   \n",
       "1     ...  26842.039628  26208.873799  25608.997523  25021.057824   \n",
       "2     ...    236.029627    190.803496    145.577365    100.351235   \n",
       "3     ...   9396.837191   9396.837191   9308.550758   9218.098497   \n",
       "4     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "...   ...           ...           ...           ...           ...   \n",
       "1923  ...      0.000000      0.000000      0.000000      0.000000   \n",
       "1924  ...      0.000000      0.000000      0.000000      0.000000   \n",
       "1925  ...      0.000000      0.000000      0.000000      0.000000   \n",
       "1926  ...  25425.963505  25425.963505  25425.963505  25425.963505   \n",
       "1927  ...      0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "            ITF796        ITF797        ITF798        ITF799        ITF800  \\\n",
       "0      1501.331023   1456.104892   1410.878762   1365.652631   1320.426500   \n",
       "1     24433.118126  23845.178427  23275.899738  22737.426345  22239.938907   \n",
       "2        55.125104      9.898973      0.000000      0.000000      0.000000   \n",
       "3      9127.646236   9037.193974   8946.741713   8856.289452   8765.837191   \n",
       "4         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1923      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1924      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1925      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1926  25425.963505  25425.963505  25425.963505  25425.963505  25425.963505   \n",
       "1927      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "      HLAB  \n",
       "0        4  \n",
       "1        4  \n",
       "2        6  \n",
       "3        7  \n",
       "4        1  \n",
       "...    ...  \n",
       "1923    10  \n",
       "1924     2  \n",
       "1925    21  \n",
       "1926    10  \n",
       "1927     3  \n",
       "\n",
       "[7367 rows x 801 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    " \n",
    "# Read data\n",
    "# data = fetch_california_housing()\n",
    "# X, y = data.data, data.target\n",
    " \n",
    "# # train-test split for model evaluation\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "#from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "#from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "# TNBC_A= pd.read_csv(r\"C:\\Users\\xwang\\Dropbox (Choate)\\Isabella Dropbox\\Topology_ST\\TNBC_Slides\\TNBC_ST_A\\TNBC_A_ITF800_HLAB_normalized_noCPM.csv\")\n",
    "# TNBC_A= TNBC_A.iloc[:, 1:]\n",
    "# TNBC_B= pd.read_csv(r\"C:\\Users\\xwang\\Dropbox (Choate)\\Isabella Dropbox\\Topology_ST\\TNBC_Slides\\TNBC_ST_B\\TNBC_B_ITF800_HLAB_normalized_noCPM.csv\")\n",
    "# TNBC_B= TNBC_B.iloc[:, 1:]\n",
    "TNBC_C= pd.read_csv(r\"C:\\Users\\xwang\\Dropbox (Choate)\\Isabella Dropbox\\Topology_ST\\TNBC_Slides\\TNBC_ST_C\\TNBC_C_ITF800_HLAB_normalized_noCPM.csv\")\n",
    "TNBC_C= TNBC_C.iloc[:, 1:]\n",
    "# TNBC_D= pd.read_csv(r\"C:\\Users\\xwang\\Dropbox (Choate)\\Isabella Dropbox\\Topology_ST\\TNBC_Slides\\TNBC_ST_D\\TNBC_D_ITF800_HLAB_normalized_noCPM.csv\")\n",
    "# TNBC_D= TNBC_D.iloc[:, 1:]\n",
    "# TNBC_ABCD=pd.concat([TNBC_A, TNBC_B, TNBC_C, TNBC_D])\n",
    "#TNBC_ABCD_selected = TNBC_ABCD.iloc[:, [200, 400, 645, 647, 648, 443, 274, 644, 245, 636, 0, 344, 3, 113, 325, 455, 446, 444, 667, 239,  800]]\n",
    "#TNBC_row_means = TNBC_ABCD_selected.mean(axis=1)\n",
    "#filtered_TNBC = TNBC_ABCD[TNBC_row_means != 0]\n",
    "#filtered_TNBC\n",
    "# X = TNBC_ABCD.iloc[:, :-1]\n",
    "# y = TNBC_ABCD.iloc[:,-1]\n",
    "X = TNBC_C.iloc[:, :-1]\n",
    "y = TNBC_C.iloc[:,-1]\n",
    "#print(X)\n",
    "#\n",
    "#TNBC_ABCD_selected\n",
    "# TNBC_ABCD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "targetscaler = MinMaxScaler()\n",
    "y_train = targetscaler.fit_transform(y_train.values.reshape(-1,1))\n",
    "y_test = targetscaler.transform(y_test.values.reshape(-1,1))\n",
    "batch_size = 65\n",
    "\n",
    "train_data = data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "train_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_data = data.TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "val_loader = data.DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        #self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        #self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        '''\n",
    "        I remove the original word embedding. This module is proposed to convert sentence (sequence of words) to numerical features.\n",
    "        Instead, I use a linear layer to encode the 800-length input.\n",
    "        Positional encoding now is optional, you can keep it or also remove it.\n",
    "        '''\n",
    "        self.encoder_embedding = nn.Linear(1, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, 1)  # Regression output\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self.generate_mask(src)\n",
    "        #src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        '''\n",
    "        use linear encoder to replace the positional word embedding\n",
    "        '''\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src.unsqueeze(-1))))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        output = self.fc(enc_output.mean(dim=1))  # Use mean pooling for regression\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\xwang\\Dropbox (Choate)\\Isabella Dropbox\\VSCODE\\TransformerV2\\TransformerAttention2Dv2.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_src, batch_tgt \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     output \u001b[39m=\u001b[39m model(batch_src, batch_src)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, batch_tgt)\n",
      "File \u001b[1;32mc:\\Users\\xwang\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\xwang\\Dropbox (Choate)\\Isabella Dropbox\\VSCODE\\TransformerV2\\TransformerAttention2Dv2.ipynb Cell 4\u001b[0m in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m enc_output \u001b[39m=\u001b[39m src_embedded\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mfor\u001b[39;00m enc_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layers:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     enc_output \u001b[39m=\u001b[39m enc_layer(enc_output, src_mask)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(enc_output\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))  \u001b[39m# Use mean pooling for regression\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\xwang\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\xwang\\Dropbox (Choate)\\Isabella Dropbox\\VSCODE\\TransformerV2\\TransformerAttention2Dv2.ipynb Cell 4\u001b[0m in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(x, x, x, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attn_output))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m ff_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(ff_output))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\xwang\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\xwang\\Dropbox (Choate)\\Isabella Dropbox\\VSCODE\\TransformerV2\\TransformerAttention2Dv2.ipynb Cell 4\u001b[0m in \u001b[0;36mPositionWiseFeedForward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xwang/Dropbox%20%28Choate%29/Isabella%20Dropbox/VSCODE/TransformerV2/TransformerAttention2Dv2.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)))\n",
      "File \u001b[1;32mc:\\Users\\xwang\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\xwang\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 12#256\n",
    "num_heads = 4\n",
    "num_layers = 6\n",
    "d_ff = 1024\n",
    "max_seq_length = 800\n",
    "'''\n",
    "If you use positional encoding, max_seq_length is useful. Otherwise, it can be any value.\n",
    "'''\n",
    "dropout = 0.1\n",
    "learning_rate = 0.01\n",
    "batch_size = 65\n",
    "num_epochs = 10\n",
    "src_vocab_size = 800#100\n",
    "\n",
    "patience = 15\n",
    "min_delta = 0.00001 # Minimum change in validation loss to be considered an improvement\n",
    "cumulative_delta = False  # Set to True if min_delta defines increase since last patience reset\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "'''\n",
    "now src_vocab_size can be any value, since the model doesn't use it after modification (removing word embedding). It is originally used to indicate the dictionary size.\n",
    "Since the transformer is proposed for language processing, a dictionary is needed to convert words to numerical features. In the word embedding layer,\n",
    "inputs are long-type vectors, where each value is the index of the word in the dictionary. And each item of the dictionary is a vector of d_model dimension,\n",
    "e.g. if d_model = 10, then the vocab_size can be at least 1024 (using binary encoding, 2**10 = 1024).\n",
    "'''\n",
    "\n",
    "# Generate sample data\n",
    "#num_samples = 32\n",
    "#src_data = torch.randint(0, src_vocab_size, (num_samples, max_seq_length))  # Replace 10000 with the max index in your source data\n",
    "#tgt_data = torch.rand(num_samples)  # Random regression target values\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "#dataset = TensorDataset(src_data, tgt_data)\n",
    "# dataset = TensorDataset(torch.from_numpy(X_scaled).float(), torch.from_numpy(y_scaled).float())\n",
    "# '''\n",
    "# The dataset should be created from your real data, not the sample one.\n",
    "# THe data you read is ndarray type, you need to convert it to tensor.\n",
    "\n",
    "# '''\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = Transformer(src_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "#@early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "epochs_no_improve = 0\n",
    "\n",
    "num_epochs = 100\n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_mse_sum = 0.0  # Initialize sum for training MSE\n",
    "    num_train_batches = 0\n",
    "    for batch_src, batch_tgt in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_src, batch_src)\n",
    "        output = output.view(-1,1)\n",
    "        loss = criterion(output, batch_tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Calculate and accumulate training MSE for this batch\n",
    "        train_mse_sum += mean_squared_error(batch_tgt, output.detach().cpu().numpy())\n",
    "        num_train_batches += 1\n",
    "    \n",
    "    # Calculate average training MSE for the epoch\n",
    "    avg_train_mse = train_mse_sum / num_train_batches\n",
    "    \n",
    "    # Apply learning rate scheduler\n",
    "    #scheduler.step()\n",
    "    \n",
    "    # Print learning rate and training MSE for this epoch\n",
    "    #current_lr = optimizer.param_groups[0]['lr']\n",
    "    #print(f\"Epoch: {epoch+1}, Learning Rate: {current_lr:.6f}, Avg. Train MSE: {avg_train_mse:.4f}\")\n",
    "    # Apply learning rate scheduler\n",
    "   #scheduler.step()\n",
    "    # Print learning rate\n",
    "    #current_lr = optimizer.param_groups[0]['lr']  # Get the current learning rate from the optimizer\n",
    "    #print(f\"Epoch: {epoch+1}, Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    model.eval()  # Switch to evaluation mode for validation\n",
    "    with torch.no_grad():\n",
    "        val_mse_sum = 0.0\n",
    "        val_mae_sum = 0.0\n",
    "        val_pearson_sum = 0.0  # Initialize Pearson sum\n",
    "        num_val_batches = 0\n",
    "        #all_val_attention_scores = []\n",
    "        \n",
    "        for batch_src, batch_tgt in val_loader:\n",
    "            val_output = model(batch_src, batch_src)\n",
    "            val_output = val_output.view(-1, 1)\n",
    "            #print(val_output.view(-1))\n",
    "            #print(batch_y.view(-1))\n",
    "            #print(val_output.shape)\n",
    "            #val_output = val_output.squeeze()  # Reshape to match batch_y shape\n",
    "            #batch_y = batch_y.view(val_output.shape)\n",
    "            val_mse_sum += mean_squared_error(batch_tgt, val_output)\n",
    "            val_mae_sum += mean_absolute_error(batch_tgt, val_output)\n",
    "            val_pearson, _ = pearsonr(batch_tgt.view(-1), val_output.view(-1))  # Calculate Pearson correlation\n",
    "            val_pearson_sum += val_pearson\n",
    "            num_val_batches += 1\n",
    "            \n",
    "            #all_val_attention_scores.extend(attn_scores_list)\n",
    "            \n",
    "\n",
    "        # Calculate average metrics over all validation batches\n",
    "        avg_val_mse = val_mse_sum / num_val_batches\n",
    "        avg_val_mae = val_mae_sum / num_val_batches\n",
    "        avg_val_pearson = val_pearson_sum / num_val_batches\n",
    "        scheduler.step(avg_val_mse)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history.append(avg_val_mse)\n",
    "        if avg_val_mse < best_mse:\n",
    "            best_mse = avg_val_mse\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # Check for early stopping\n",
    "    if avg_val_mse < best_val_loss:\n",
    "        best_val_loss = avg_val_mse\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "       # best_val_loss = avg_val_mse\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve == patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "    print(f\"Epoch: {epoch+1}, Learning Rate: {current_lr:.6f}, Avg. Train MSE: {avg_train_mse:.4f}\")\n",
    "    print(f\"Epoch: {epoch+1}, Avg. Val MSE: {avg_val_mse:.4f}, Avg. Val MAE: {avg_val_mae:.4f}, Avg. Val Pearson: {avg_val_pearson:.4f}\")\n",
    "   \n",
    "\n",
    "# restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MSE: %.2f\" % best_mse)\n",
    "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
